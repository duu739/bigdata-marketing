import streamlit as st
import pandas as pd
import numpy as np
import cv2
import requests
import plotly.express as px
import plotly.graph_objects as go
from googleapiclient.discovery import build
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from openai import OpenAI

# ==========================================
# 1. API é‡‘é‘°èˆ‡åˆå§‹åŒ–
# ==========================================
OPENAI_API_KEY = "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"
YOUTUBE_API_KEY = "AIzaSyDTWvLm7NJ24_4PdY7uK3JDAsodISYbIx0"
client = OpenAI(api_key=OPENAI_API_KEY)

def get_youtube_service():
    return build("youtube", "v3", developerKey=YOUTUBE_API_KEY)

# ==========================================
# 2. å½±åƒè¾¨è­˜æ¨¡çµ„
# ==========================================
def analyze_advanced_vision(img_array):
    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
    if img is None: return None

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    äº®åº¦ = np.mean(gray)
    å°æ¯”åº¦ = np.std(gray)
    é£½å’Œåº¦ = np.mean(hsv[:, :, 1])

    face_cascade_path = "haarcascade_frontalface_default.xml"
    face_cascade = cv2.CascadeClassifier(face_cascade_path)
    æœ‰è‡‰ = 0
    if not face_cascade.empty():
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        æœ‰è‡‰ = 1 if len(faces) > 0 else 0

    edges = cv2.Canny(gray, 100, 200)
    è¤‡é›œåº¦ = np.sum(edges > 0) / (gray.shape[0] * gray.shape[1]) * 100

    return [äº®åº¦, å°æ¯”åº¦, é£½å’Œåº¦, è¤‡é›œåº¦, æœ‰è‡‰]

def analyze_title_features(title):
    é•·åº¦ = len(title)
    å«æ•¸å­— = 1 if any(char.isdigit() for char in title) else 0
    å«æ¨™é» = 1 if any(p in title for p in ['?', '!', 'ï¼Ÿ', 'ï¼']) else 0
    return [é•·åº¦, å«æ•¸å­—, å«æ¨™é»]

# ==========================================
# 3. K-means è‡ªå‹•åˆ†ç¾¤
# ==========================================
def calculate_clustering_logic(df):
    feature_cols = ['äº®åº¦','å°æ¯”åº¦','é£½å’Œåº¦','è¤‡é›œåº¦','æœ‰è‡‰']
    X = df[feature_cols]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    wcss = []
    max_k = min(len(df), 10)
    for i in range(1, max_k + 1):
        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        wcss.append(kmeans.inertia_)

    # Elbow Method é¸æœ€ä½³ k
    diff = np.diff(wcss)
    diff_ratio = diff[1:] / diff[:-1]
    optimal_k = np.argmax(diff_ratio < 0.5) + 2  # è‡ªå‹•é¸ç¾¤æ•¸
    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)
    df['é¢¨æ ¼'] = kmeans.fit_predict(X_scaled)
    return df, wcss, optimal_k

def generate_cluster_titles(df, client):
    cluster_titles = {}
    for cluster_id in df['é¢¨æ ¼'].unique():
        cluster_data = df[df['é¢¨æ ¼'] == cluster_id]
        avg_feats = cluster_data[['äº®åº¦','å°æ¯”åº¦','é£½å’Œåº¦','è¤‡é›œåº¦','æœ‰è‡‰']].mean().to_dict()
        sample_titles = cluster_data['å½±ç‰‡æ¨™é¡Œ'].head(5).tolist()  

        prompt = f"""
        ä½ æ˜¯ä¸€ä½ YouTube åˆ†æå°ˆå®¶ã€‚æ ¹æ“šä»¥ä¸‹ç¾¤çµ„å½±ç‰‡ç‰¹å¾µï¼š
        å¹³å‡äº®åº¦ï¼š{avg_feats['äº®åº¦']:.1f}ï¼Œå°æ¯”åº¦ï¼š{avg_feats['å°æ¯”åº¦']:.1f}ï¼Œé£½å’Œåº¦ï¼š{avg_feats['é£½å’Œåº¦']:.1f}ï¼Œ
        è¤‡é›œåº¦ï¼š{avg_feats['è¤‡é›œåº¦']:.1f}%ï¼Œæœ‰è‡‰ç‡ï¼š{avg_feats['æœ‰è‡‰']*100:.1f}%ï¼Œ
        æ¨™é¡Œç¯„ä¾‹ï¼š{', '.join(sample_titles)}

        è«‹çµ¦é€™å€‹ç¾¤çµ„å–ä¸€å€‹ç²¾ç°¡æœ‰è¶£ã€å¸å¼•äººçš„ã€Œç¾¤æ¨™é¡Œã€ï¼Œä¸è¶…é5å€‹å­—ã€‚
        """
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"system","content":"ä½ æ˜¯ä¸€ä½å°ˆæ¥­ YouTube åˆ†æå¸«ã€‚"},
                      {"role":"user","content":prompt}]
        )
        cluster_titles[cluster_id] = completion.choices[0].message.content.strip()
    return cluster_titles

# ==========================================
# 4. Streamlit ä»‹é¢
# ==========================================
st.set_page_config(page_title="YouTube ç¸®åœ–æ¨™é¡Œé†«ç”Ÿ", layout="wide")

with st.sidebar:
    st.title("ğŸ¯ è¨ºæ–·æ§åˆ¶å°")
    user_topic = st.text_input("æœå°‹ä¸»é¡Œ", "iPhone 16 é–‹ç®±")
    user_title = st.text_input("é è¨ˆæ¨™é¡Œ", "é€™æ”¯æ‰‹æ©ŸçœŸçš„å€¼å¾—è²·å—ï¼Ÿ")
    num_videos = st.slider("æ¨£æœ¬æ•¸é‡", 20, 50, 30)
    st.divider()
    uploaded_file = st.file_uploader("ä¸Šå‚³ä½ çš„ç¸®åœ–", type=["jpg","png","jpeg"])
    start_analysis = st.button("ğŸš€ åŸ·è¡Œæ·±åº¦åˆ†æ")

st.title("ğŸ©º YouTube çˆ†ç´…åŸºå› è¨ºæ–·å®¤")

if start_analysis and user_topic and uploaded_file and user_title:
    with st.spinner(f"æ­£åœ¨åˆ†æã€Œ{user_topic}ã€çš„å¸‚å ´çˆ†ç´…åŸºå› ..."):
        try:
            # å¸‚å ´è³‡æ–™æŠ“å–ï¼ˆé•·å½±ç‰‡ >3åˆ†é˜ï¼‰
            youtube = get_youtube_service()
            search_res = youtube.search().list(
                q=user_topic, type="video", part="id,snippet", maxResults=num_videos, order="viewCount",
                videoDuration="medium"
            ).execute()
            v_ids = [item['id']['videoId'] for item in search_res['items']]
            v_stats = youtube.videos().list(
                id=','.join(v_ids), part="snippet,statistics,contentDetails"
            ).execute()

            market_records = []
            for v in v_stats['items']:
                thumb_url = v['snippet']['thumbnails'].get('high', v['snippet']['thumbnails'].get('default'))['url']
                resp = requests.get(thumb_url).content
                vision_feats = analyze_advanced_vision(np.frombuffer(resp, np.uint8))
                title_feats = analyze_title_features(v['snippet']['title'])
                if vision_feats:
                    duration = v['contentDetails']['duration']
                    duration_sec = int(pd.to_timedelta(duration).total_seconds())
                    video_url = f"https://www.youtube.com/watch?v={v['id']}"
                    market_records.append(
                        vision_feats + title_feats + [int(v['statistics'].get('viewCount',0)), duration_sec, video_url, v['snippet']['title']]
                    )

            df = pd.DataFrame(market_records, columns=[
                'äº®åº¦','å°æ¯”åº¦','é£½å’Œåº¦','è¤‡é›œåº¦','æœ‰è‡‰',
                'æ¨™é¡Œé•·åº¦','æ¨™é¡Œå«æ•¸å­—','æ¨™é¡Œå«æ¨™é»','è§€çœ‹æ•¸','å½±ç‰‡ç§’æ•¸','å½±ç‰‡é€£çµ','å½±ç‰‡æ¨™é¡Œ'
            ])

            # åˆ†ç¾¤
            df, wcss, optimal_k = calculate_clustering_logic(df)
            cluster_titles = generate_cluster_titles(df, client)
            df['é¢¨æ ¼æ¨™é¡Œ'] = df['é¢¨æ ¼'].map(cluster_titles)

            # å¸‚å ´è¶¨å‹¢åœ–è¡¨
            st.subheader("ğŸ“Š å¸‚å ´è¶¨å‹¢åˆ†æ")
            col_viz1, col_viz2 = st.columns(2)
            with col_viz1:
                st.markdown("**Elbow Method**ï¼šæ©«è»¸ Kï¼Œç¸±è»¸ WCSSï¼Œè½‰æŠ˜é»å¯åˆ¤æ–·æœ€ä½³åˆ†ç¾¤æ•¸ã€‚")
                fig_elbow = px.line(x=range(1,len(wcss)+1),y=wcss,title="WCSS è½‰æŠ˜é»åˆ†æ (Elbow)",labels={'x':'K','y':'WCSS'})
                fig_elbow.update_traces(mode='lines+markers')
                st.plotly_chart(fig_elbow,use_container_width=True)

            with col_viz2:
                st.markdown("**å¸‚å ´è¦–è¦ºæ°£æ³¡åœ–**ï¼šX=å°æ¯”åº¦ï¼ŒY=é£½å’Œåº¦ï¼Œæ°£æ³¡=è§€çœ‹æ•¸ï¼Œé¡è‰²=åˆ†ç¾¤é¢¨æ ¼ã€‚")
                fig_bubble = px.scatter(
                    df,x='å°æ¯”åº¦',y='é£½å’Œåº¦',size='è§€çœ‹æ•¸',color='é¢¨æ ¼æ¨™é¡Œ',
                    title="å¸‚å ´è¦–è¦ºåˆ†ä½ˆ (æ°£æ³¡å¤§å°=è§€çœ‹æ•¸)"
                )
                st.plotly_chart(fig_bubble,use_container_width=True)

            # å€‹äººè¨ºæ–·
            user_img_bytes = uploaded_file.read()
            user_vision = analyze_advanced_vision(np.frombuffer(user_img_bytes, np.uint8))
            user_title_info = analyze_title_features(user_title)

            st.divider()
            st.subheader("ğŸ©º è¨ºæ–·å ±å‘Šï¼šä½  vs å¸‚å ´å¹³å‡")
            diag_col1, diag_col2 = st.columns([1.5,1])
            with diag_col1:
                categories = ['äº®åº¦','å°æ¯”åº¦','é£½å’Œåº¦','è¤‡é›œåº¦','æœ‰è‡‰']
                m_avg_v = df[['äº®åº¦','å°æ¯”åº¦','é£½å’Œåº¦','è¤‡é›œåº¦','æœ‰è‡‰']].mean().values
                fig_radar = go.Figure()
                fig_radar.add_trace(go.Scatterpolar(r=user_vision,theta=categories,fill='toself',name='ä½ çš„ç¸®åœ–',line_color='red'))
                fig_radar.add_trace(go.Scatterpolar(r=m_avg_v,theta=categories,fill='toself',name='å¸‚å ´å¹³å‡',line_color='blue'))
                fig_radar.update_layout(polar=dict(radialaxis=dict(visible=True,range=[0,255])),width=600,height=600,title="è¦–è¦ºç‰¹å¾µé›·é”å°æ¯”")
                st.plotly_chart(fig_radar,use_container_width=True)

            with diag_col2:
                comparison_df = pd.DataFrame({
                    "æŒ‡æ¨™":["ç¸®åœ–äº®åº¦","å°æ¯”åº¦","é£½å’Œåº¦","è¦–è¦ºè¤‡é›œåº¦","ç¸®åœ–å«äººè‡‰","æ¨™é¡Œé•·åº¦","æ¨™é¡Œå«æ•¸å­—","æ¨™é¡Œå«æ¨™é»"],
                    "ä½ çš„æ•¸å€¼":[f"{user_vision[0]:.1f}",f"{user_vision[1]:.1f}",f"{user_vision[2]:.1f}",f"{user_vision[3]:.1f}%","æ˜¯" if user_vision[4] else "å¦",
                                f"{user_title_info[0]} å­—","æ˜¯" if user_title_info[1] else "å¦","æ˜¯" if user_title_info[2] else "å¦"],
                    "å¸‚å ´å¹³å‡":[f"{df['äº®åº¦'].mean():.1f}",f"{df['å°æ¯”åº¦'].mean():.1f}",f"{df['é£½å’Œåº¦'].mean():.1f}",f"{df['è¤‡é›œåº¦'].mean():.1f}%",
                               f"{df['æœ‰è‡‰'].mean()*100:.1f}%","{:.1f} å­—".format(df['æ¨™é¡Œé•·åº¦'].mean()),
                               "{:.1f}%".format(df['æ¨™é¡Œå«æ•¸å­—'].mean()*100),"{:.1f}%".format(df['æ¨™é¡Œå«æ¨™é»'].mean()*100)]
                })
                st.table(comparison_df)

            # åŸå§‹è³‡æ–™
            st.divider()
            st.subheader("ğŸ“„ å¸‚å ´åŸå§‹è³‡æ–™")
            st.dataframe(df.rename(columns={
                'äº®åº¦':'äº®åº¦','å°æ¯”åº¦':'å°æ¯”åº¦','é£½å’Œåº¦':'é£½å’Œåº¦','è¤‡é›œåº¦':'è¦–è¦ºè¤‡é›œåº¦','æœ‰è‡‰':'å«äººè‡‰',
                'æ¨™é¡Œé•·åº¦':'æ¨™é¡Œé•·åº¦','æ¨™é¡Œå«æ•¸å­—':'æ¨™é¡Œå«æ•¸å­—','æ¨™é¡Œå«æ¨™é»':'æ¨™é¡Œå«æ¨™é»',
                'è§€çœ‹æ•¸':'è§€çœ‹æ•¸','å½±ç‰‡ç§’æ•¸':'å½±ç‰‡é•·åº¦(ç§’)','é€£çµ':'å½±ç‰‡é€£çµ','å½±ç‰‡æ¨™é¡Œ':'å½±ç‰‡æ¨™é¡Œ','é¢¨æ ¼æ¨™é¡Œ':'åˆ†ç¾¤é¢¨æ ¼'
            }))

            # AI å»ºè­°
            st.divider()
            st.subheader("ğŸ¤– AI ç‡Ÿé‹å°ˆå®¶å»ºè­°")
            ai_spinner = st.empty()
            ai_spinner.info("AI æ­£åœ¨åˆ†æä¸­ï¼Œè«‹ç¨å€™...")
            prompt = f"""
            ä½ æ˜¯ä¸€ä½ YouTube å¢é•·å°ˆå®¶ï¼Œå°ˆæ³¨æ–¼ç¸®åœ–èˆ‡æ¨™é¡Œå„ªåŒ–ã€‚
            ä¸»é¡Œï¼š{user_topic}

            å¸‚å ´å¹³å‡ï¼š
            å°æ¯”åº¦ {df['å°æ¯”åº¦'].mean():.1f}ï¼Œé£½å’Œåº¦ {df['é£½å’Œåº¦'].mean():.1f}ï¼Œè¤‡é›œåº¦ {df['è¤‡é›œåº¦'].mean():.1f}%ï¼Œæœ‰è‡‰ç‡ {df['æœ‰è‡‰'].mean()*100:.1f}%
            æ¨™é¡Œå¹³å‡é•·åº¦ {df['æ¨™é¡Œé•·åº¦'].mean():.1f} å­—ï¼Œå«æ•¸å­— {df['æ¨™é¡Œå«æ•¸å­—'].mean()*100:.1f}%ï¼Œå«æ¨™é» {df['æ¨™é¡Œå«æ¨™é»'].mean()*100:.1f}%

            ä½¿ç”¨è€…ï¼š
            ç¸®åœ–äº®åº¦ {user_vision[0]:.1f}ï¼Œå°æ¯” {user_vision[1]:.1f}ï¼Œé£½å’Œåº¦ {user_vision[2]:.1f}ï¼Œè¤‡é›œåº¦ {user_vision[3]:.1f}%ï¼Œæœ‰è‡‰ {"æœ‰" if user_vision[4] else "ç„¡"}
            æ¨™é¡Œï¼š{user_title}ï¼Œé•·åº¦ {user_title_info[0]}ï¼Œå«æ•¸å­— {"æ˜¯" if user_title_info[1] else "å¦"}ï¼Œå«æ¨™é» {"æ˜¯" if user_title_info[2] else "å¦"}

            è«‹çµ¦å‡º 3 å€‹å…·é«”å¯åŸ·è¡Œçš„å„ªåŒ–å»ºè­°ï¼š
            1) å°ç¸®åœ–çš„äº®åº¦ã€å°æ¯”ã€é£½å’Œåº¦ã€è¤‡é›œåº¦èˆ‡äººè‡‰ä½¿ç”¨
            2) å°æ¨™é¡Œçš„é•·åº¦ã€æ•¸å­—èˆ‡æ¨™é»ä½¿ç”¨
            3) èªæ°£å¯ä»¥çŠ€åˆ©ä½†å‹™å¿…å…·é«”
            """
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"system","content":"ä½ æ˜¯ä¸€ä½å°ˆæ¥­ YouTube åˆ†æå¸«ï¼Œçµ¦å‡ºå…·é«”å¯åŸ·è¡Œå»ºè­°ï¼Œç¹é«”ä¸­æ–‡å›ç­”ã€‚"},
                          {"role":"user","content":prompt}]
            )
            ai_spinner.empty()
            st.info(completion.choices[0].message.content)

        except Exception as e:
            st.error(f"åˆ†æå¤±æ•—ï¼š{str(e)}")
else:
    st.info("ğŸ’¡ æº–å‚™å°±ç·’ï¼è«‹åœ¨å·¦å´è¼¸å…¥ä¸»é¡Œä¸¦ä¸Šå‚³ç¸®åœ–ã€‚")
